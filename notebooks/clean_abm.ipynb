{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import holidays\n",
    "import os\n",
    "\n",
    "#############################################################################################################\n",
    "### FUNCTIONS\n",
    "#############################################################################################################\n",
    "\n",
    "### ---------------------------------------------------------------------------------------------------------\n",
    "### Load parquet files (should have 12 months worth of data)\n",
    "### ---------------------------------------------------------------------------------------------------------\n",
    "\n",
    "def load_parquet_files(directory):\n",
    "    files = [f for f in os.listdir(directory) if f.endswith('.parquet')]\n",
    "    print(\"Available parquet files (\" + str(len(files)) + \"): \", files)\n",
    "    df = pd.concat([pd.read_parquet(os.path.join(directory, f)) for f in files], ignore_index=True)\n",
    "    df.columns = df.columns.str.strip().str.replace(' ', '_').str.lower()\n",
    "    return df\n",
    "\n",
    "### ---------------------------------------------------------------------------------------------------------\n",
    "### General data cleaning (remove duplicates, fill NAs, etc)\n",
    "### ---------------------------------------------------------------------------------------------------------\n",
    "\n",
    "def clean_and_filter_columns(df, columns, delay_cols):\n",
    "    df = df.dropna(axis=1, how='all')\n",
    "    df = df[[col for col in columns if col in df.columns]].drop_duplicates()\n",
    "    df[delay_cols] = df[delay_cols].fillna(0)\n",
    "    df[delay_cols] = df[delay_cols].astype(int)\n",
    "    return df\n",
    "\n",
    "### ---------------------------------------------------------------------------------------------------------\n",
    "### Extract hour (0–23) directly from HHMM-formatted time (e.g., 1420 → 14)\n",
    "### ---------------------------------------------------------------------------------------------------------\n",
    "\n",
    "def extract_hour_from_hhmm(df, colname, new_colname):\n",
    "    df[colname] = pd.to_numeric(df[colname], errors='coerce')  # ensure numeric\n",
    "    df[new_colname] = (df[colname] // 100).astype('Int64')     # supports NA\n",
    "    return df\n",
    "\n",
    "### ---------------------------------------------------------------------------------------------------------\n",
    "### Filter to only 50 US states & DC (excludes Canadian and other US territories)\n",
    "### ---------------------------------------------------------------------------------------------------------\n",
    "\n",
    "def filter_valid_states(df, valid_states):\n",
    "    return df[df['originstate'].isin(valid_states) & df['deststate'].isin(valid_states)].copy()\n",
    "\n",
    "### ---------------------------------------------------------------------------------------------------------\n",
    "### Filter to top 200 airports based on combined arrival and departures\n",
    "### ---------------------------------------------------------------------------------------------------------\n",
    "\n",
    "def get_top_airports(df, n=200):\n",
    "    origin = df['origin'].value_counts()\n",
    "    dest = df['dest'].value_counts()\n",
    "    combined = origin.add(dest, fill_value=0)\n",
    "    return combined.nlargest(n).index\n",
    "\n",
    "def filter_by_top_airports(df, top_airports):\n",
    "    return df[\n",
    "        df['origin'].isin(top_airports) & df['dest'].isin(top_airports)\n",
    "    ].copy()\n",
    "\n",
    "### ---------------------------------------------------------------------------------------------------------\n",
    "### Create categorical feature for proximity to major US holidays\n",
    "### ---------------------------------------------------------------------------------------------------------\n",
    "\n",
    "def add_holiday_features(df):\n",
    "\n",
    "    # Step 1: Convert to datetime\n",
    "    df['flight_date'] = pd.to_datetime(df[['year', 'month', 'dayofmonth']].rename(columns={'dayofmonth': 'day'}))\n",
    "\n",
    "    # Step 2: Major holidays and codes\n",
    "    us_holidays = holidays.US(years=df['year'].unique())\n",
    "\n",
    "    major_holidays = {\n",
    "        \"New Year's Day\": \"A\",\n",
    "        \"Memorial Day\": \"B\",\n",
    "        \"Independence Day\": \"C\",\n",
    "        \"Labor Day\": \"D\",\n",
    "        \"Thanksgiving Day\": \"E\",\n",
    "        \"Christmas Day\": \"F\"\n",
    "    }\n",
    "\n",
    "    # Filter to relevant holiday dates and codes\n",
    "    holiday_info = [\n",
    "        (pd.Timestamp(date), code)\n",
    "        for date, name in us_holidays.items()\n",
    "        if name in major_holidays\n",
    "        for code in [major_holidays[name]]\n",
    "    ]\n",
    "\n",
    "    if not holiday_info:\n",
    "        df['holiday_proximity_bucket'] = 5\n",
    "        df['holiday_code'] = 'NA'\n",
    "        return df\n",
    "\n",
    "    # Step 3: Build holiday date array\n",
    "    holiday_dates = np.array([d[0] for d in holiday_info], dtype='datetime64[D]')\n",
    "    holiday_codes = np.array([d[1] for d in holiday_info])\n",
    "\n",
    "    # Step 4: Calculate days difference (vectorized)\n",
    "    flight_dates = df['flight_date'].values.astype('datetime64[D]')\n",
    "    date_diffs = flight_dates[:, None] - holiday_dates[None, :]  # shape (N_flights, N_holidays)\n",
    "    delta_days = np.abs(date_diffs.astype('timedelta64[D]').astype(int))  # in days\n",
    "\n",
    "    # Step 5: Find nearest holiday within 7 days\n",
    "    min_diff = np.min(delta_days, axis=1)\n",
    "    min_idx = np.argmin(delta_days, axis=1)\n",
    "\n",
    "    # Step 6: Assign bucket based on delta\n",
    "    bucket = np.full(len(df), 5)  # Default: 5 = not near holiday\n",
    "    bucket[min_diff == 0] = 1\n",
    "    bucket[(min_diff == 1)] = 2\n",
    "    bucket[(min_diff >= 2) & (min_diff <= 3)] = 3\n",
    "    bucket[(min_diff >= 4) & (min_diff <= 7)] = 4\n",
    "\n",
    "    # Step 7: Assign holiday code (or NA if not within range)\n",
    "    code = np.array(['NA'] * len(df), dtype=object)\n",
    "    within_range = min_diff <= 7\n",
    "    code[within_range] = holiday_codes[min_idx[within_range]]\n",
    "\n",
    "    # Step 8: Assign to dataframe\n",
    "    df['holiday_proximity_bucket'] = bucket\n",
    "    df['holiday_code'] = code\n",
    "\n",
    "    return df\n",
    "\n",
    "#############################################################################################################\n",
    "### CALL MAIN\n",
    "#############################################################################################################\n",
    "\n",
    "# delays defined as more than 15 minutes\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    cols = ['year', 'month', 'dayofmonth', 'dayofweek', 'origin', 'dest', 'reporting_airline', \n",
    "        'originstate', 'deststate', 'crsdeptime', 'crsarrtime','carrierdelay', 'weatherdelay', \n",
    "        'nasdelay', 'securitydelay', 'lateaircraftdelay', 'arrdelayminutes', 'cancelled', 'diverted']\n",
    "    delay_cols = ['carrierdelay', 'weatherdelay', 'nasdelay', 'securitydelay', \n",
    "        'lateaircraftdelay', 'arrdelayminutes']\n",
    "    state_list = ['AK','AL','AR','AZ','CA','CO','CT','DC','DE','FL','GA','HI','IA','ID','IL','IN',\n",
    "        'KS','KY','LA','MA','MD','ME','MI','MN','MO','MS','MT','NC','ND','NE','NH','NJ','NM','NV',\n",
    "        'NY','OH','OK','OR','PA','RI','SC','SD','TN','TX','UT','VA','VT','WA','WI','WV','WY']\n",
    "\n",
    "    df_raw = load_parquet_files(\"../data/raw\")\n",
    "    df_clean = clean_and_filter_columns(df_raw, cols, delay_cols)\n",
    "    df_filtered = filter_valid_states(df_clean, state_list)\n",
    "    df_filtered = df_filtered.drop(columns=['originstate', 'deststate'])\n",
    "    df_filtered = extract_hour_from_hhmm(df_filtered, 'crsdeptime', 'dep_hour')\n",
    "    df_filtered = extract_hour_from_hhmm(df_filtered, 'crsarrtime', 'arr_hour')\n",
    "    df_filtered = df_filtered.drop(columns=['crsdeptime', 'crsarrtime'])\n",
    "    df_filtered = add_holiday_features(df_filtered)\n",
    "    df_filtered = df_filtered.drop(columns=['year', 'flight_date'])\n",
    "    df_filtered['if_delay'] = np.where(df_filtered['arrdelayminutes'] <= 15, '0', '1').astype(int)\n",
    "    df_filtered['if_cancelled'] = np.where(df_filtered['cancelled'] == 0, '0', '1').astype(int)\n",
    "    df_filtered['if_diverted'] = np.where(df_filtered['diverted'] == 0, '0', '1').astype(int)\n",
    "    df_filtered = df_filtered.drop(columns=['cancelled', 'diverted'])\n",
    "    df_filtered['nonweatherdelay'] = df_filtered['arrdelayminutes'] - df_filtered['weatherdelay']\n",
    "    top_airports = get_top_airports(df_filtered)\n",
    "    df_final = filter_by_top_airports(df_filtered, top_airports)\n",
    "    \n",
    "    print(\"✅ FINAL DATASET CREATED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available parquet files (12):  ['flight_data_2024_9.parquet', 'flight_data_2025_3.parquet', 'flight_data_2024_8.parquet', 'flight_data_2025_2.parquet', 'flight_data_2025_1.parquet', 'flight_data_2025_4.parquet', 'flight_data_2024_7.parquet', 'flight_data_2024_12.parquet', 'flight_data_2025_5.parquet', 'flight_data_2024_11.parquet', 'flight_data_2025_6.parquet', 'flight_data_2024_10.parquet']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import holidays\n",
    "import os\n",
    "\n",
    "#############################################################################################################\n",
    "### FUNCTIONS\n",
    "#############################################################################################################\n",
    "\n",
    "### ---------------------------------------------------------------------------------------------------------\n",
    "### Load parquet files (should have 12 months worth of data)\n",
    "### ---------------------------------------------------------------------------------------------------------\n",
    "\n",
    "def load_parquet_files(directory):\n",
    "    files = [f for f in os.listdir(directory) if f.endswith('.parquet')]\n",
    "    print(\"Available parquet files (\" + str(len(files)) + \"): \", files)\n",
    "    df = pd.concat([pd.read_parquet(os.path.join(directory, f)) for f in files], ignore_index=True)\n",
    "    df.columns = df.columns.str.strip().str.replace(' ', '_').str.lower()\n",
    "    return df\n",
    "\n",
    "df = load_parquet_files(\"../data/raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################################################\n",
    "### SUMMARY STATISTICS DATASET\n",
    "#############################################################################################################\n",
    "\n",
    "summary_cols = ['holiday_code', 'dayofweek']\n",
    "\n",
    "df_summary = df_final.copy().groupby(summary_cols).agg(\n",
    "    total_flights = ('if_delay', 'count'),\n",
    "    delayed_flights = ('if_delay', 'sum'),\n",
    "    cancelled_flights = ('if_cancelled', 'sum'),\n",
    "    diverted_flights = ('if_diverted', 'sum'),\n",
    "    total_delay_minutes = ('arrdelayminutes', 'sum'),\n",
    "    delay_minutes_75th = ('arrdelayminutes', lambda x: int(x.quantile(0.75))),\n",
    "    delay_minutes_90th = ('arrdelayminutes', lambda x: int(x.quantile(0.90))),\n",
    "    delay_minutes_95th = ('arrdelayminutes', lambda x: int(x.quantile(0.95))),\n",
    "    delay_minutes_99th = ('arrdelayminutes', lambda x: int(x.quantile(0.99)))\n",
    ").reset_index()\n",
    "\n",
    "# df_summary.to_parquet('../data/processed/summary_dataset.parquet')\n",
    "\n",
    "df_summary.sort_values(by=\"delay_minutes_95th\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################################################\n",
    "### MACHINE LEARNING DATASET\n",
    "#############################################################################################################\n",
    "\n",
    "ml_cols = ['month', 'dayofweek', 'origin', 'dest', 'reporting_airline', 'dep_hour', 'holiday_code', 'holiday_proximity_bucket', 'arrdelayminutes']\n",
    "\n",
    "df_ml = df_final[ml_cols].copy()\n",
    "\n",
    "df_ml.to_parquet('../data/processed/ml_dataset.parquet')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "summary_sandbox_cols = ['if_near_holiday']\n",
    "\n",
    "df_summary_sandbox = df_final.copy().groupby(summary_sandbox_cols).agg(\n",
    "    total_flights = ('if_delay', 'count'),\n",
    "    delayed_flights = ('if_delay', 'sum'),\n",
    "    cancelled_flights = ('if_cancelled', 'sum'),\n",
    "    diverted_flights = ('if_diverted', 'sum'),\n",
    "    total_delay_minutes = ('arrdelayminutes', 'sum'),\n",
    "    delay_minutes_90th = ('arrdelayminutes', lambda x: int(x.quantile(0.90))),\n",
    "    delay_minutes_95th = ('arrdelayminutes', lambda x: int(x.quantile(0.95))),\n",
    "    delay_minutes_99th = ('arrdelayminutes', lambda x: int(x.quantile(0.99))),\n",
    ").reset_index()\n",
    "\n",
    "df_summary_sandbox['avg_delay'] = df_summary_sandbox['total_delay_minutes'] / df_summary_sandbox['delayed_flights']\n",
    "df_summary_sandbox['delay_percent'] = (100 * df_summary_sandbox['delayed_flights'] / df_summary_sandbox['total_flights']).round(1)\n",
    "df_summary_sandbox['delay_percent_str'] = (100 * df_summary_sandbox['delayed_flights'] / df_summary_sandbox['total_flights']).round(1).astype(str) + '%'\n",
    "df_summary_sandbox['cancelled_percent'] = (100 * df_summary_sandbox['cancelled_flights'] / df_summary_sandbox['total_flights']).round(1).astype(str) + '%'\n",
    "df_summary_sandbox['diverted_percent'] = (100 * df_summary_sandbox['diverted_flights'] / df_summary_sandbox['total_flights']).round(1).astype(str) + '%'\n",
    "\n",
    "df_summary_sandbox.sort_values(by='delay_percent').tail(20)\n",
    "\n",
    "#df_summary_sandbox[df_summary_sandbox['origin'] == \"STS\"].sort_values(by='delay_percent')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12.3 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b42aa77ed5f5b42884b87ca53fed36318530525cc8445095b22cbadc9646f115"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
